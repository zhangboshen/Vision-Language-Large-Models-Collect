# Recent Advances in Vision-Language Large Models



- **CLIP**,  "[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)", ICML 2021, [[Code](https://github.com/openai/CLIP)]

- **VLMo**, "[VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://arxiv.org/abs/2111.02358)", arXiv 2022. [[Code](https://github.com/microsoft/unilm/tree/master/vlmo)]

- **BEiT3**, "[Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/abs/2208.10442)", arXiv 2022. [[Code](https://github.com/microsoft/unilm)]
- **CoCa** "[CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/abs/2205.01917)", arXiv 2022. [[Code](https://github.com/lucidrains/CoCa-pytorch)]
  
- **BLIP Family** [[Code](https://github.com/salesforce/LAVIS)]
  - **BLIP**, "[Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation](https://proceedings.mlr.press/v162/li22n.html)", ICML 2022.
  - **BLIP2**, "[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)", arXiv 2023.
  - **InstructBLIP**, "[InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)", arXiv 2023.
  - **X-InstructBLIP**, "[X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning](https://arxiv.org/pdf/2311.18799.pdf)", arXiv 2023.
 
- **LLaVA Family** 
   - **LLaVA**, "[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)", NeurIPS 2023. [[Code](https://github.com/haotian-liu/LLaVA)]
   - **LLaVa-1.5**, "[Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)", arXiv 2023. [[Code](https://github.com/haotian-liu/LLaVA)]
   - **Video-LLaVa**, [[Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122)], arXiv 2023, [[Code](https://github.com/PKU-YuanGroup/Video-LLaVA)]
   - **MoE-LLaVA**, [[MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2401.15947)], arXiv 2024. [[Code](https://github.com/PKU-YuanGroup/MoE-LLaVA/tree/main)]
- **MiniGPT-V Family** 
  - **MiniGPT-4**, "[MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592)", arXiv 2023. [[Code](https://github.com/Vision-CAIR/MiniGPT-4)]
  - **MiniGPT-v2**, "[MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478)", arXiv 2023. [[Code](https://github.com/Vision-CAIR/MiniGPT-4)]
  - **MiniGPT-5**, "[MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/abs/2310.02239)", arXiv 2023. [[code](https://github.com/eric-ai-lab/MiniGPT-5)]
- **QWEN-VL**, "[Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)", [[Code](https://github.com/QwenLM/Qwen-VL)]
- **Emu Family** [[Code](https://github.com/baaivision/Emu)]
  - **Emu1**, "[Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222)", arXiv 2023.
  - **Emu2**, "[Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286)" arXiv 2023.
- **Yi-VL**, [[Code](https://github.com/01-ai/Yi/tree/main/VL)]
- **Ferret** "[Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704)", arXiv 2023. [[Code](https://github.com/apple/ml-ferret)]
- **CogVLM**, "[CogVLM: Visual Expert for Pretrained Language Models](https://arxiv.org/abs/2311.03079)", arXiv 2023. [[Code](https://github.com/THUDM/CogVLM/tree/main)]

- **VTimeLLM**, "[VTimeLLM: Empower LLM to Grasp Video Moments](https://arxiv.org/abs/2311.18445)", arXiv 2023. [[Code](https://github.com/huangb23/VTimeLLM)]
- **TimeChat**, "[TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding](https://arxiv.org/abs/2312.02051)", arXiv 2023. [[Code](https://github.com/RenShuhuai-Andy/TimeChat)]
- **InternLM-Xcomposer**, "[InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition](https://arxiv.org/abs/2309.15112)", arXiv 2023. [[Code](https://github.com/InternLM/InternLM-XComposer)]
- **VisCPM**, "[Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages](https://arxiv.org/pdf/2308.12038.pdf)", arXiv 2023. [[Code](https://github.com/OpenBMB/VisCPM)]


- **LWM**. "[World Model on Million-Length Video And Language With RingAttention
](https://arxiv.org/pdf/2402.08268.pdf)", arXiv 2024. [[Code](https://github.com/LargeWorldModel/LWM?tab=readme-ov-file)]
